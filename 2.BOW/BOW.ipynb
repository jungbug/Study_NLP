{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카운트 벡터(Count Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am', 'a', 'elementary', 'school', 'student'], ['And', 'I', 'am', 'a', 'boy']]\n"
     ]
    }
   ],
   "source": [
    "text = [\"I am a elementary school student\", \"And I am a boy\"]\n",
    "\n",
    "from nltk import word_tokenize\n",
    "text_tokenzied = [word_tokenize(sentence) for sentence in text]\n",
    "print(text_tokenzied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 먼저 토큰화하고, 단어의 빈도수를 세어 벡터로 만드는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'I': 2, 'am': 2, 'a': 2, 'elementary': 1, 'school': 1, 'student': 1, 'And': 1, 'boy': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "for sentence in text_tokenzied:\n",
    "    vocab_counter.update(sentence)\n",
    "    \n",
    "print(vocab_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2, 2, 1, 1, 1], [1, 2, 2, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "text_count_vector = []\n",
    "\n",
    "for sentence in text_tokenzied:\n",
    "    sentence_vector = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        sentence_vector.append(vocab_counter[word])\n",
    "    text_count_vector.append(sentence_vector)\n",
    "    \n",
    "print(text_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장들을 원하는 카운터 벡터로 변환합니다. 여기서 각 단어들이 빈도수로 대체되는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "'TF-IDF'의 기본적인 개념은 \"위의 단순 빈도수 문제를 개선하기위해 단어의 빈도수에 역 빈도수를 곱해준다\"는 것입니다.\n",
    "\n",
    " \n",
    "\n",
    "### 1. TF(d, f) : 특정 문서 d에서의 특정 단어 t의 등장 횟수\n",
    "\n",
    "     예를 들어 문서 [I am a great great boy]가 있을 때 각 단어의 'TF'는 [I, am, a , great, boy] == [1, 1, 1, 2, 1]이 됩니다.\n",
    "\n",
    " \n",
    "\n",
    "### 2. df(t) : 특정 단어 t가 등장한 문서의 수\n",
    "\n",
    "     예를들어 문서 1 [I am a great great boy], 문서 2 [I am a boy]가 있을 때 'df'는 [I, am, a , great, boy] == [2, 2, 2, 1, 2]이 됩니다. 여기서 great == 1로 정의되는데 그 이유는 great이 문서 1에서 2번 사용되었지만, 사용된 문서수는 문서 1 하나이므로 1로 대응됩니다.\n",
    "\n",
    " \n",
    "\n",
    "### 3. idf(d, t) : df(t)에 반비례하는 수\n",
    "    \n",
    "     idf(d, t) = log(n / (1 + df(t))) 이며, n은 전체 문서의 수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I am a great great elementary school student\", \"And I am a boy\"]\n",
    "\n",
    "# Word tokenized sentence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_tokenzied = [word_tokenize(sentence) for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['am', 'great', 'great', 'elementary', 'school', 'student'], ['And', 'am', 'boy']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords that are too short.\n",
    "text_tokenzied2 = []\n",
    "for sentence in text_tokenzied:\n",
    "    sent = []\n",
    "    for word in sentence:\n",
    "        if len(word) >= 2:\n",
    "            sent.append(word)\n",
    "    text_tokenzied2.append(sent)\n",
    "\n",
    "print(text_tokenzied2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary list\n",
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "for sentence in text_tokenzied2:\n",
    "    vocab_counter.update(sentence)\n",
    "\n",
    "vocab = []\n",
    "for key, value in vocab_counter.items():\n",
    "    vocab.append(key)\n",
    "    \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'great': 2, 'am': 1, 'elementary': 1, 'school': 1, 'student': 1}), Counter({'And': 1, 'am': 1, 'boy': 1})]\n"
     ]
    }
   ],
   "source": [
    "# Count word of each sentence\n",
    "from collections import Counter\n",
    "\n",
    "count = []\n",
    "for sentence in text_tokenzied2:\n",
    "    vocab_counter = Counter()\n",
    "    vocab_counter.update(sentence)\n",
    "    count.append(vocab_counter)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " 'Counter()'를 이용해 각 문서별로 단어수들을 취합합니다. 위와 같이 문서 2개에 해당하는 count라는 list가 만들어진 것을 보실 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[1, 2, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def TF(vocab, counter):\n",
    "    vector = []\n",
    "    for word in vocab:\n",
    "        if counter[word] != False:\n",
    "            vector.append(counter[word])\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "\n",
    "print(vocab)\n",
    "print(TF(vocab, count[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def DF(text_tokenzied2, vocab):\n",
    "    text = []\n",
    "    for sentence in text_tokenzied2:\n",
    "        for word in list(set(sentence)):\n",
    "            text.append(word)\n",
    "    vocab_counter = Counter()\n",
    "    vocab_counter.update(text)\n",
    "    \n",
    "    df = []\n",
    "    for word in vocab:\n",
    "        df.append(vocab_counter[word])\n",
    "    return df\n",
    "\n",
    "print(vocab)\n",
    "print(DF(text_tokenzied2, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[0.5945348918918356, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def IDF(df, n):\n",
    "    idf = []\n",
    "    for i in df:\n",
    "        idf.append(math.log((n)/(i+1))+1)\n",
    "    return idf\n",
    "\n",
    "print(vocab)\n",
    "print(IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[0.5945348918918356, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c = count[1]\n",
    "\n",
    "def TFIDF(tf, idf):\n",
    "    product = [x*y for x, y in zip(tf, idf)]\n",
    "    return product\n",
    "\n",
    "print(vocab)\n",
    "print(TFIDF(TF(vocab, c), IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[[0.5945348918918356, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.5945348918918356, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = []\n",
    "for c in count:\n",
    "    tfidf.append(TFIDF(TF(vocab, c), IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2))))\n",
    "\n",
    "print(vocab)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25969799 0.         0.         0.36499647 0.72999294 0.36499647\n",
      "  0.36499647]\n",
      " [0.44943642 0.6316672  0.6316672  0.         0.         0.\n",
      "  0.        ]]\n",
      "{'am': 0, 'great': 4, 'elementary': 3, 'school': 5, 'student': 6, 'and': 1, 'boy': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(text)\n",
    "print(tfidfv.transform(text).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
